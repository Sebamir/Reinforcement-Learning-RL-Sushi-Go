üç£ Reinforcement Learning - Sushi Go Master

Este repositorio contiene la implementaci√≥n de un agente de Aprendizaje por Refuerzo (RL) entrenado para jugar Sushi Go de forma profesional.
A diferencia de un bot basado en reglas, este agente utiliza PPO (Proximal Policy Optimization) y Self-Play para descubrir estrategias ganadoras por s√≠ mismo.

üöÄ Caracter√≠sticas Principales

Entorno de Juego Robusto
Implementaci√≥n personalizada bajo el est√°ndar de Gymnasium.

L√≥gica de Dependencia Temporal
El agente entiende que el Wasabi es una inversi√≥n a futuro, buscando maximizar el combo con el Nigiri de Calamar:
3√ó3=9 pts.

Competencia Multi-Agente
Sistema de puntuaci√≥n de Maki Rolls que incentiva a la IA a monitorear el progreso de los oponentes para asegurar el bono de mayor√≠a.

Entrenamiento Optimizado
Sistema de callbacks que gestiona evaluaciones peri√≥dicas, guarda el mejor modelo hist√≥rico y genera m√©tricas de rendimiento visuales.

üß† El Cerebro de la IA: Detalles T√©cnicos
El Reto del Wasabi

El Wasabi representa el problema cl√°sico de recompensa retardada en RL.
Jugar un Wasabi otorga 0 puntos inmediatos, pero triplica el siguiente Nigiri.

Para resolver esto:

Espacio de Observaci√≥n Extendido
Se a√±adieron bits de estado que indican si el jugador tiene un Wasabi Activo.

El Algoritmo PPO

Se utiliza Proximal Policy Optimization (PPO) debido a su estabilidad en entornos donde la pol√≠tica de juego cambia r√°pidamente (Self-Play).

üè• Conexi√≥n con el Mundo Real: Aplicaciones en Salud

Este proyecto no es solo sobre sushi; es una simulaci√≥n de Toma de Decisiones Secuenciales bajo Incertidumbre, un campo cr√≠tico en la salud moderna:

Sinergia Farmacol√≥gica
La l√≥gica del Wasabi (una carta que potencia a otra) es an√°loga al modelado de tratamientos adyuvantes.
La IA aprende cu√°ndo una intervenci√≥n preparatoria maximiza la eficacia de una terapia principal posterior.

Triaje y Recursos Cr√≠ticos
La competencia por los Makis simula la asignaci√≥n de recursos limitados en un hospital.
La IA decide si ‚Äúinvertir‚Äù en un paciente/plato bas√°ndose en lo que el resto del sistema (oponentes) est√° haciendo.

Medicina Personalizada
El entrenamiento mediante Self-Play vuelve al modelo robusto ante distintos estilos de paciente (estrategias), permitiendo adaptarse a comportamientos no lineales en datos biom√©tricos.
